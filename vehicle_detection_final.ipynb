{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/envs/carnd-term1/lib/python3.5/site-packages/sklearn/cross_validation.py:44: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "import glob\n",
    "import matplotlib.image as mpimg\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import cv2\n",
    "import glob\n",
    "\n",
    "from sklearn.svm import LinearSVC, SVC\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from skimage.feature import hog\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from time import time\n",
    "from scipy.ndimage.measurements import label\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from moviepy.editor import VideoFileClip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Get images of cars and non-cars to train the classifier\n",
    "cars = list(glob.iglob('./data/vehicles/**/*.png', recursive=True))\n",
    "notcars = list(glob.iglob('./data/non-vehicles/**/*.png', recursive=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Define a function to return HOG features and visualization\n",
    "def get_hog_features(img, orient, pix_per_cell, cell_per_block, \n",
    "                        vis=False, feature_vec=True):\n",
    "    # Call with two outputs if vis==True\n",
    "    if vis == True:\n",
    "        features, hog_image = hog(img, orientations=orient, \n",
    "                                  pixels_per_cell=(pix_per_cell, pix_per_cell),\n",
    "                                  cells_per_block=(cell_per_block, cell_per_block), \n",
    "                                  transform_sqrt=True, \n",
    "                                  visualise=vis, feature_vector=feature_vec)\n",
    "        return features, hog_image\n",
    "    # Otherwise call with one output\n",
    "    else:      \n",
    "        features = hog(img, orientations=orient, \n",
    "                       pixels_per_cell=(pix_per_cell, pix_per_cell),\n",
    "                       cells_per_block=(cell_per_block, cell_per_block), \n",
    "                       transform_sqrt=True, \n",
    "                       visualise=vis, feature_vector=feature_vec)\n",
    "        return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Define a function to compute binned color features  \n",
    "def bin_spatial(img, size=(32, 32)):\n",
    "    # Use cv2.resize().ravel() to create the feature vector\n",
    "    features = cv2.resize(img, size).ravel() \n",
    "    # Return the feature vector\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Define a function to compute color histogram features \n",
    "# NEED TO CHANGE bins_range if reading .png files with mpimg!\n",
    "def color_hist(img, nbins=32, bins_range=(0, 256)):\n",
    "    # Compute the histogram of the color channels separately\n",
    "    channel1_hist = np.histogram(img[:,:,0], bins=nbins, range=bins_range)\n",
    "    channel2_hist = np.histogram(img[:,:,1], bins=nbins, range=bins_range)\n",
    "    channel3_hist = np.histogram(img[:,:,2], bins=nbins, range=bins_range)\n",
    "    # Concatenate the histograms into a single feature vector\n",
    "    hist_features = np.concatenate((channel1_hist[0], channel2_hist[0], channel3_hist[0]))\n",
    "    # Return the individual histograms, bin_centers and feature vector\n",
    "    return hist_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Define a function to extract features from a list of images\n",
    "# Have this function call bin_spatial() and color_hist()\n",
    "def extract_features(imgs, color_space='RGB', spatial_size=(32, 32),\n",
    "                        hist_bins=32, orient=9, \n",
    "                        pix_per_cell=8, cell_per_block=2, hog_channel=0,\n",
    "                        spatial_feat=True, hist_feat=True, hog_feat=True):\n",
    "    # Create a list to append feature vectors to\n",
    "    features = []\n",
    "    # Iterate through the list of images\n",
    "    for file in imgs:\n",
    "        file_features = []\n",
    "        # Read in each one by one\n",
    "        image = mpimg.imread(file)\n",
    "        # apply color conversion if other than 'RGB'\n",
    "        if color_space != 'RGB':\n",
    "            if color_space == 'HSV':\n",
    "                feature_image = cv2.cvtColor(image, cv2.COLOR_RGB2HSV)\n",
    "            elif color_space == 'LUV':\n",
    "                feature_image = cv2.cvtColor(image, cv2.COLOR_RGB2LUV)\n",
    "            elif color_space == 'HLS':\n",
    "                feature_image = cv2.cvtColor(image, cv2.COLOR_RGB2HLS)\n",
    "            elif color_space == 'YUV':\n",
    "                feature_image = cv2.cvtColor(image, cv2.COLOR_RGB2YUV)\n",
    "            elif color_space == 'YCrCb':\n",
    "                feature_image = cv2.cvtColor(image, cv2.COLOR_RGB2YCrCb)\n",
    "        else: feature_image = np.copy(image)      \n",
    "\n",
    "        if spatial_feat == True:\n",
    "            spatial_features = bin_spatial(feature_image, size=spatial_size)\n",
    "            file_features.append(spatial_features)\n",
    "        if hist_feat == True:\n",
    "            # Apply color_hist()\n",
    "            hist_features = color_hist(feature_image, nbins=hist_bins)\n",
    "            file_features.append(hist_features)\n",
    "        if hog_feat == True:\n",
    "        # Call get_hog_features() with vis=False, feature_vec=True\n",
    "            if hog_channel == 'ALL':\n",
    "                hog_features = []\n",
    "                for channel in range(feature_image.shape[2]):\n",
    "                    hog_features.append(get_hog_features(feature_image[:,:,channel], \n",
    "                                        orient, pix_per_cell, cell_per_block, \n",
    "                                        vis=False, feature_vec=True))\n",
    "                hog_features = np.ravel(hog_features)        \n",
    "            else:\n",
    "                hog_features = get_hog_features(feature_image[:,:,hog_channel], orient, \n",
    "                            pix_per_cell, cell_per_block, vis=False, feature_vec=True)\n",
    "            # Append the new feature vector to the features list\n",
    "            file_features.append(hog_features)\n",
    "        features.append(np.concatenate(file_features))\n",
    "    # Return list of feature vectors\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "55.06 Seconds to extract features...\n"
     ]
    }
   ],
   "source": [
    "# Let's extract the features now to train the classifier.\n",
    "# The following settings worked well.\n",
    "\n",
    "color_space = 'YCrCb' \n",
    "spatial_size = (16, 16)\n",
    "orient = 11\n",
    "pix_per_cell = 16\n",
    "cell_per_block = 2\n",
    "hog_channel = 'ALL' \n",
    "hist_bins = 64\n",
    "spatial_feat=True\n",
    "hist_feat=True\n",
    "hog_feat=True\n",
    "\n",
    "t = time()\n",
    "\n",
    "# Get car and non-car features and corresponding classes\n",
    "\n",
    "car_features = extract_features(imgs=cars, \n",
    "                                color_space=color_space, \n",
    "                                spatial_size=spatial_size,\n",
    "                                hist_bins=hist_bins,\n",
    "                                orient=orient,\n",
    "                                pix_per_cell=pix_per_cell,\n",
    "                                cell_per_block=cell_per_block,\n",
    "                                hog_channel=hog_channel,\n",
    "                                spatial_feat=spatial_feat,\n",
    "                                hist_feat=hist_feat,\n",
    "                                hog_feat=hog_feat)\n",
    "  \n",
    "notcar_features = extract_features(imgs=notcars, \n",
    "                                color_space=color_space, \n",
    "                                spatial_size=spatial_size,\n",
    "                                hist_bins=hist_bins,\n",
    "                                orient=orient,\n",
    "                                pix_per_cell=pix_per_cell,\n",
    "                                cell_per_block=cell_per_block,\n",
    "                                hog_channel=hog_channel,\n",
    "                                spatial_feat=spatial_feat,\n",
    "                                hist_feat=hist_feat,\n",
    "                                hog_feat=hog_feat)    \n",
    "\n",
    "t2 = time()\n",
    "print(round(t2-t, 2), 'Seconds to extract features...')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Create an array stack of feature vectors\n",
    "X = np.vstack((car_features, notcar_features)).astype(np.float64)                        \n",
    "# Fit a per-column scaler\n",
    "X_scaler = StandardScaler().fit(X)\n",
    "# Apply the scaler to X\n",
    "scaled_X = X_scaler.transform(X)\n",
    "\n",
    "# Define the labels vector\n",
    "y = np.hstack((np.ones(len(car_features)), np.zeros(len(notcar_features))))\n",
    "\n",
    "# Split up data into randomized training and test sets\n",
    "rand_state = 42\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    scaled_X, y, test_size=0.2, random_state=rand_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using: 11 orientations 16 pixels per cell and 2 cells per block\n",
      "Feature vector length: 2148\n",
      "1.53 Seconds to train SVC...\n",
      "Test Accuracy of SVC =  0.989\n",
      "My SVC predicts:  [ 0.  0.  0.  1.  1.  1.  1.  0.  0.  0.]\n",
      "For these 10 labels:  [ 0.  0.  0.  1.  1.  1.  1.  0.  0.  0.]\n",
      "0.00205 Seconds to predict 10 labels with SVC\n"
     ]
    }
   ],
   "source": [
    "print('Using:',orient,'orientations',pix_per_cell,\n",
    "    'pixels per cell and', cell_per_block,'cells per block')\n",
    "print('Feature vector length:', len(X_train[0]))\n",
    "\n",
    "# Let's train a linear SVM\n",
    "svc = LinearSVC(random_state=42, penalty='l2', C=0.001, loss='squared_hinge', dual=True)\n",
    "\n",
    "# Fit the model\n",
    "t = time()\n",
    "svc.fit(X_train, y_train)\n",
    "t2 = time()\n",
    "print(round(t2-t, 2), 'Seconds to train SVC...')\n",
    "\n",
    "# Check the score of the SVC\n",
    "print('Test Accuracy of SVC = ', round(svc.score(X_test, y_test), 4))\n",
    "\n",
    "# Check the prediction time for a single sample\n",
    "t = time()\n",
    "n_predict = 10\n",
    "print('My SVC predicts: ', svc.predict(X_test[0:n_predict]))\n",
    "print('For these',n_predict, 'labels: ', y_test[0:n_predict])\n",
    "t2 = time()\n",
    "print(round(t2-t, 5), 'Seconds to predict', n_predict,'labels with SVC')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Define a function that takes an image,\n",
    "# start and stop positions in both x and y, \n",
    "# window size (x and y dimensions),  \n",
    "# and overlap fraction (for both x and y)\n",
    "def slide_window(img, x_start_stop=[None, None], y_start_stop=[None, None], \n",
    "                    xy_window=(64, 64), xy_overlap=(0.5, 0.5)):\n",
    "    # If x and/or y start/stop positions not defined, set to image size\n",
    "    if x_start_stop[0] == None:\n",
    "        x_start_stop[0] = 0\n",
    "    if x_start_stop[1] == None:\n",
    "        x_start_stop[1] = img.shape[1]\n",
    "    if y_start_stop[0] == None:\n",
    "        y_start_stop[0] = 0\n",
    "    if y_start_stop[1] == None:\n",
    "        y_start_stop[1] = img.shape[0]\n",
    "    # Compute the span of the region to be searched    \n",
    "    xspan = x_start_stop[1] - x_start_stop[0]\n",
    "    yspan = y_start_stop[1] - y_start_stop[0]\n",
    "    # Compute the number of pixels per step in x/y\n",
    "    nx_pix_per_step = np.int(xy_window[0]*(1 - xy_overlap[0]))\n",
    "    ny_pix_per_step = np.int(xy_window[1]*(1 - xy_overlap[1]))\n",
    "    # Compute the number of windows in x/y\n",
    "    nx_buffer = np.int(xy_window[0]*(xy_overlap[0]))\n",
    "    ny_buffer = np.int(xy_window[1]*(xy_overlap[1]))\n",
    "    nx_windows = np.int((xspan-nx_buffer)/nx_pix_per_step) \n",
    "    ny_windows = np.int((yspan-ny_buffer)/ny_pix_per_step) \n",
    "    # Initialize a list to append window positions to\n",
    "    window_list = []\n",
    "    # Loop through finding x and y window positions\n",
    "    # Note: you could vectorize this step, but in practice\n",
    "    # you'll be considering windows one by one with your\n",
    "    # classifier, so looping makes sense\n",
    "    for ys in range(ny_windows):\n",
    "        for xs in range(nx_windows):\n",
    "            # Calculate window position\n",
    "            startx = xs*nx_pix_per_step + x_start_stop[0]\n",
    "            endx = startx + xy_window[0]\n",
    "            starty = ys*ny_pix_per_step + y_start_stop[0]\n",
    "            endy = starty + xy_window[1]\n",
    "            # Append window position to list\n",
    "            window_list.append(((startx, starty), (endx, endy)))\n",
    "    # Return the list of windows\n",
    "    return window_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Define a function to extract features from a single image window\n",
    "# This function is very similar to extract_features()\n",
    "# just for a single image rather than list of images\n",
    "def single_img_features(img, color_space='RGB', spatial_size=(32, 32),\n",
    "                        hist_bins=32, orient=9, \n",
    "                        pix_per_cell=8, cell_per_block=2, hog_channel=0,\n",
    "                        spatial_feat=True, hist_feat=True, hog_feat=True):    \n",
    "    #1) Define an empty list to receive features\n",
    "    img_features = []\n",
    "    #2) Apply color conversion if other than 'RGB'\n",
    "    if color_space != 'RGB':\n",
    "        if color_space == 'HSV':\n",
    "            feature_image = cv2.cvtColor(img, cv2.COLOR_RGB2HSV)\n",
    "        elif color_space == 'LUV':\n",
    "            feature_image = cv2.cvtColor(img, cv2.COLOR_RGB2LUV)\n",
    "        elif color_space == 'HLS':\n",
    "            feature_image = cv2.cvtColor(img, cv2.COLOR_RGB2HLS)\n",
    "        elif color_space == 'YUV':\n",
    "            feature_image = cv2.cvtColor(img, cv2.COLOR_RGB2YUV)\n",
    "        elif color_space == 'YCrCb':\n",
    "            feature_image = cv2.cvtColor(img, cv2.COLOR_RGB2YCrCb)\n",
    "    else: feature_image = np.copy(img)      \n",
    "    #3) Compute spatial features if flag is set\n",
    "    if spatial_feat == True:\n",
    "        spatial_features = bin_spatial(feature_image, size=spatial_size)\n",
    "        #4) Append features to list\n",
    "        img_features.append(spatial_features)\n",
    "    #5) Compute histogram features if flag is set\n",
    "    if hist_feat == True:\n",
    "        hist_features = color_hist(feature_image, nbins=hist_bins)\n",
    "        #6) Append features to list\n",
    "        img_features.append(hist_features)\n",
    "    #7) Compute HOG features if flag is set\n",
    "    if hog_feat == True:\n",
    "        if hog_channel == 'ALL':\n",
    "            hog_features = []\n",
    "            for channel in range(feature_image.shape[2]):\n",
    "                hog_features.extend(get_hog_features(feature_image[:,:,channel], \n",
    "                                    orient, pix_per_cell, cell_per_block, \n",
    "                                    vis=False, feature_vec=True))      \n",
    "        else:\n",
    "            hog_features = get_hog_features(feature_image[:,:,hog_channel], orient, \n",
    "                        pix_per_cell, cell_per_block, vis=False, feature_vec=True)\n",
    "        #8) Append features to list\n",
    "        img_features.append(hog_features)\n",
    "    #9) Return concatenated array of features\n",
    "    return np.concatenate(img_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Define a function to draw bounding boxes\n",
    "def draw_boxes(img, bboxes, color=(0, 0, 255), thick=6):\n",
    "    # Make a copy of the image\n",
    "    imcopy = np.copy(img)\n",
    "    # Iterate through the bounding boxes\n",
    "    for bbox in bboxes:\n",
    "        # Draw a rectangle given bbox coordinates\n",
    "        cv2.rectangle(imcopy, bbox[0], bbox[1], color, thick)\n",
    "    # Return the image copy with boxes drawn\n",
    "    return imcopy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Define a function you will pass an image\n",
    "# and the list of windows to be searched (output of slide_windows())\n",
    "def search_windows(img, windows, clf, scaler, color_space='RGB',\n",
    "                   spatial_size=(32, 32), hist_bins=32,\n",
    "                   hist_range=(0, 256), orient=9,\n",
    "                   pix_per_cell=8, cell_per_block=2,\n",
    "                   hog_channel=0, spatial_feat=True,\n",
    "                   hist_feat=True, hog_feat=True):\n",
    "    # 1) Create an empty list to receive positive detection windows\n",
    "    on_windows = []\n",
    "    # 2) Iterate over all windows in the list\n",
    "    for window in windows:\n",
    "        # 3) Extract the test window from original image\n",
    "        test_img = cv2.resize(img[window[0][1]:window[1][1], window[0][0]:window[1][0]], (64, 64))\n",
    "        # 4) Extract features for that window using single_img_features()\n",
    "        features = single_img_features(test_img, color_space=color_space,\n",
    "                                       spatial_size=spatial_size, hist_bins=hist_bins,\n",
    "                                       orient=orient, pix_per_cell=pix_per_cell,\n",
    "                                       cell_per_block=cell_per_block,\n",
    "                                       hog_channel=hog_channel, spatial_feat=spatial_feat,\n",
    "                                       hist_feat=hist_feat, hog_feat=hog_feat)\n",
    "        # 5) Scale extracted features to be fed to classifier\n",
    "        test_features = scaler.transform(np.array(features).reshape(1, -1))\n",
    "        # 6) Predict using your classifier\n",
    "        # print test_features\n",
    "        prediction = clf.predict(test_features)\n",
    "        # print prediction\n",
    "        # 7) If positive (prediction == 1) then save the window\n",
    "        if prediction == 1:\n",
    "            on_windows.append(window)\n",
    "    # 8) Return windows for positive detections\n",
    "    return on_windows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def add_heat(heatmap, bbox_list):\n",
    "    # Iterate through list of bboxes\n",
    "    for box in bbox_list:\n",
    "        # Add += 1 for all pixels inside each bbox\n",
    "        # Assuming each \"box\" takes the form ((x1, y1), (x2, y2))\n",
    "        heatmap[box[0][1]:box[1][1], box[0][0]:box[1][0]] += 1\n",
    "    # Return updated heatmap\n",
    "    return heatmap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def apply_threshold(heatmap, threshold):\n",
    "    # Zero out pixels below the threshold\n",
    "    heatmap[heatmap <= threshold] = 0\n",
    "    # Return thresholded map\n",
    "    return heatmap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def draw_labeled_bboxes(img, labels):\n",
    "    # Iterate through all detected cars\n",
    "    for car_number in range(1, labels[1]+1):\n",
    "        # Find pixels with each car_number label value\n",
    "        nonzero = (labels[0] == car_number).nonzero()\n",
    "        # Identify x and y values of those pixels\n",
    "        nonzeroy = np.array(nonzero[0])\n",
    "        nonzerox = np.array(nonzero[1])\n",
    "        # Define a bounding box based on min/max x and y\n",
    "        bbox = ((np.min(nonzerox), np.min(nonzeroy)), (np.max(nonzerox), np.max(nonzeroy)))\n",
    "        # Draw the box on the image\n",
    "        cv2.rectangle(img, bbox[0], bbox[1], color=(0,255,0), thickness=6)\n",
    "    # Return the image\n",
    "    return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Pipeline to draw bounding boxes around cars for individual images.\n",
    "# It's a class with a method to do the pipeline application rather \n",
    "# than a simple function, because we need to encapsulate bounding box \n",
    "# history from the past N (e.g. 15) image frames. Otherwise, history would\n",
    "# be a global variable, or a state for a coroutine, which would be less\n",
    "# desirable if multiple instances of the pipeline were to be used, e.g.\n",
    "# for testing. The reason for the bouding box history from multiple frames\n",
    "# is that spurious detections are likely to be transient (if our classifier\n",
    "# is trained correctly), while correct detections will span multiple frames\n",
    "# as the detected car moves slowly around the image frame. So, with multiple\n",
    "# frames, we can detect boxes at multiple frames, merge them if they are close\n",
    "# to each other, and raise the detection threshold. This will keep the\n",
    "# consistent detections and eliminate the false positives.\n",
    "class Pipeline:\n",
    "    \n",
    "    # The constructor takes a list of window functions (that take an image and\n",
    "    # apply sliding windows with different start-stop (x, y) ranges in the image,\n",
    "    # different window overlaps, etc., as well as a time window (for how many\n",
    "    # consecutive images to count the bounding boxes to be thresholded and merged),\n",
    "    # the threshold to be applied, and the heat minimum and maximum clipping.\n",
    "    def __init__(\n",
    "        self,\n",
    "        window_funs,\n",
    "        time_window = 15,\n",
    "        app_threshold = 9,\n",
    "        heat_clip_min = 0,\n",
    "        heat_clip_max = 255\n",
    "    ):\n",
    "        self.time_window = time_window\n",
    "        self.app_threshold = app_threshold\n",
    "        self.heat_clip_min = heat_clip_min\n",
    "        self.heat_clip_max = heat_clip_max\n",
    "        self.window_funs = window_funs\n",
    "        self.box_history = []\n",
    "        \n",
    "    # Pipeline application - makes the object callable, i.e.\n",
    "    # we can pass my_object that's as a lambda rather than passing\n",
    "    # my_object.apply_pipeline as a lambda to the video function.\n",
    "    def __call__(self, image):\n",
    "        \n",
    "        # Make a copy, don't mutate the orginal image. We'll need\n",
    "        # this image later to draw boxes on it. The copy we make below\n",
    "        # of the scaled image is to calculate the boxes, but we'll need\n",
    "        # this original to draw the boxes on later.\n",
    "        orig_img = np.copy(image)\n",
    "        # SVM training and the original pipeline was done based on\n",
    "        # PNGs, which are provided by Matplotlib image library in\n",
    "        # the range [0, 1] as floats, but the video library provides\n",
    "        # JPEGs and reads them as uint8 with a range [0, 255], so \n",
    "        # we need this conversion here.\n",
    "        image = image.astype(np.float32) / 255\n",
    "        # Again, we don't want to mutate the underlying image.\n",
    "        draw_image = np.copy(image)\n",
    "        # Pre-allocate memory for the heat map.\n",
    "        heat = np.zeros_like(draw_image[:, :, 0]).astype(np.float)\n",
    "        # Create the first list of sliding windows. The list\n",
    "        # of lambdas to apply may be longer than one item, \n",
    "        # but we need to initialize with the first list produced\n",
    "        # by the first lambda to concatenate the window list later.\n",
    "        windows = self.window_funs[0](image)\n",
    "        # If there are more lambdas for the sliding windows to apply,\n",
    "        # loop over them and then concatenate the window list.\n",
    "        for win in range(1, len(self.window_funs)):\n",
    "            windows = windows + self.window_funs[win](image)\n",
    "        \n",
    "        # Once we have the window boundaries, let's get the features\n",
    "        # for these windows and apply the classifier.\n",
    "        hot_windows = search_windows(draw_image, windows, svc,\n",
    "                                     X_scaler, color_space=color_space,\n",
    "                                     spatial_size=spatial_size,\n",
    "                                     hist_bins=hist_bins,\n",
    "                                     orient=orient, \n",
    "                                     pix_per_cell=pix_per_cell,\n",
    "                                     cell_per_block=cell_per_block,\n",
    "                                     hog_channel=hog_channel,\n",
    "                                     spatial_feat=spatial_feat,\n",
    "                                     hist_feat=hist_feat,\n",
    "                                     hog_feat=hog_feat)\n",
    "        \n",
    "        # Let's skip the first 30 frames. Technically we can\n",
    "        # just skip the first N frames, where N is the time_window\n",
    "        # value (15 for the successful experiment), but here I skip\n",
    "        # 30 because there are some artifacts in the first second of\n",
    "        # the video when there are no cars present.\n",
    "        if len(self.box_history) < 30:\n",
    "            self.box_history.append(hot_windows)\n",
    "            return draw_image\n",
    "        # One the image history has been initialized,\n",
    "        # start drawing boxes.\n",
    "        else:\n",
    "            # Since we have the history of boxes that's full now,\n",
    "            # we need to drop the oldest frame, add the latest frame\n",
    "            # and predict over the moving time window.\n",
    "            self.box_history = self.box_history[1:]\n",
    "            self.box_history.append(hot_windows)\n",
    "            # Accumulate list of boxes of the the time_window\n",
    "            # number of video frames. Not doing this on just\n",
    "            # one frame allows us to raise the detection threshold\n",
    "            # since with a well-trained classifier, false positives\n",
    "            # are likely to be transient, while true positives are\n",
    "            # likely to persist across frames, and the cars don't\n",
    "            # move fast enough for the detections to be counted\n",
    "            # as separate.\n",
    "            previous_bboxes = []\n",
    "            for bbox in self.box_history:\n",
    "                previous_bboxes.extend(bbox)\n",
    "            # Generate a heat map across the bounding box history.\n",
    "            heat = add_heat(heat, previous_bboxes)\n",
    "            # Threshold detections to remove false positives.\n",
    "            heat = apply_threshold(heat, self.app_threshold)\n",
    "            heatmap = np.clip(heat, self.heat_clip_min, self.heat_clip_max)\n",
    "            # Create labels of overlapping bounding boxes.\n",
    "            labels = label(heatmap)\n",
    "            # draw bounding boxes on the original image\n",
    "            labeled_image = draw_labeled_bboxes(orig_img, labels)\n",
    "            # Return annotated image\n",
    "            return labeled_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Apply the image pipeline lambda to a video clip\n",
    "def find_cars_in_video(input_video_path, pipeline, output_video_path):\n",
    "    clip = VideoFileClip(input_video_path)\n",
    "    clip_with_lines = clip.fl_image(pipeline)\n",
    "    clip_with_lines.write_videofile(output_video_path, audio=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# These are the multi-region, multi-resolution sliding window\n",
    "# functions to be applied to each image. The windows they\n",
    "# generate will have the features extracted, the classifier will\n",
    "# be applied to them, and the heat maps will be generated to \n",
    "# determine the final bounding boxes for individual cars.\n",
    "window_funs = [\n",
    "    lambda image: slide_window(image, x_start_stop=[None, None], y_start_stop=[400, 640],\n",
    "                                xy_window= (128, 128), xy_overlap=(0.5, 0.5)),\n",
    "    lambda image: slide_window(image, x_start_stop=[32, None], y_start_stop=[400, 600],\n",
    "                                xy_window=(96, 96), xy_overlap=(0.5, 0.5)),\n",
    "    lambda image: slide_window(image, x_start_stop=[412, 1280], y_start_stop=[390, 540],\n",
    "                                xy_window=(80, 80), xy_overlap=(0.5, 0.5))\n",
    "]\n",
    "\n",
    "# Construct the pipeline. This will contain the state of the\n",
    "# history of windows from the last time_window frames, and\n",
    "# other config for the pipeline lambda provided to the\n",
    "# find_cars_in_video function. We could use a generator \n",
    "# instead, but that wouldn't make it easy to create several\n",
    "# stateful pipelines at once.\n",
    "pipeline = Pipeline(\n",
    "    window_funs,\n",
    "    time_window = 15,\n",
    "    app_threshold = 9,\n",
    "    heat_clip_min = 0,\n",
    "    heat_clip_max = 255\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[MoviePy] >>>> Building video tracking.mp4\n",
      "[MoviePy] Writing video tracking.mp4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████▉| 1260/1261 [09:13<00:00,  2.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[MoviePy] Done.\n",
      "[MoviePy] >>>> Video ready: tracking.mp4 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Create the video with bounding boxes\n",
    "# drawn on top of the original video frames.\n",
    "find_cars_in_video(\n",
    "    input_video_path='project_video.mp4', \n",
    "    pipeline=pipeline, \n",
    "    output_video_path='tracking.mp4'\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "widgets": {
   "state": {},
   "version": "1.1.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
